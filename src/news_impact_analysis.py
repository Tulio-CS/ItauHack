"""Tools for analysing the impact of news on stock tickers.

This module implements a multi-stage pipeline inspired by the user's
specification:

1. A relevance filter driven by an LLM classifies each article as
   *Market-Moving*, *Fluff/Marketing* or *Irrelevant*.
2. For market-moving articles, the LLM produces a structured JSON event
   description that captures metrics, expectations and sentiment.
3. A chain-of-thought style prompt extracts an impact score (1-10) and a
   natural-language justification, enabling richer numerical features.
4. News items are grouped in near-real time via clustering so the model can
   reason about information dissemination (FinGPT idea).
5. Price history is downloaded automatically (via yfinance) and used to
   compute forward returns over multiple horizons (1d, 3d, 5d).
6. A gradient boosted tree model (XGBoost) learns the relationship between
   extracted features and realised returns.

The resulting pipeline can be used to train a model or to score incoming
articles for a specific ticker.

The code is deliberately modular so that each step can be swapped or extended
in the future.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

import numpy as np
import pandas as pd
from dateutil import parser
from sklearn.cluster import MiniBatchKMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBRegressor

try:  # Optional dependency – only required when using the real API
    from openai import OpenAI
except Exception:  # pragma: no cover - fallback when OpenAI SDK is missing
    OpenAI = None  # type: ignore

try:  # yfinance may not be installed at import time
    import yfinance as yf
except Exception:  # pragma: no cover - allow the module to be imported without it
    yf = None  # type: ignore


LOGGER = logging.getLogger(__name__)


MARKET_MOVING_PROMPT = """
Classifique a notícia abaixo em uma das três categorias: Market-Moving,
Fluff/Marketing ou Irrelevante. Use a seguinte definição:
- Market-Moving: notícias que podem impactar diretamente o valuation da empresa
  (lucros, guidance, lançamento relevante, processos, demissões, fusões etc.).
- Fluff/Marketing: divulgação promocional, entrevistas genéricas, filantropia
  sem impacto financeiro direto.
- Irrelevante: textos que nada têm a ver com a empresa ou o mercado financeiro.

Responda apenas com o rótulo exato (Market-Moving, Fluff/Marketing ou
Irrelevante).

Notícia:
{conteudo}
""".strip()


EVENT_EXTRACTION_PROMPT = """
Você é um analista financeiro. Converta a notícia abaixo para um JSON que siga
exatamente o seguinte formato (sem comentários adicionais):
{
  "evento_tipo": "...",
  "metricas": [
    {"metrica": "...", "valor": <float ou null>, "expectativa": <float ou null>, "resultado": "beat/miss/in-line/na"}
  ],
  "sentimento_geral": "positivo/negativo/misto/neutro"
}

Para valores numéricos, use ponto como separador decimal e somente números sem
símbolos adicionais. Caso alguma informação não exista, use null e "na".

Notícia:
{conteudo}
""".strip()


IMPACT_RATING_PROMPT = """
Analise a notícia abaixo passo a passo:
1. Identifique o evento principal.
2. Avalie o impacto potencial no ticker em uma escala de 1 (nenhum impacto)
   a 10 (impacto máximo).
3. Justifique em duas frases objetivas o motivo da nota.

Responda estritamente no formato JSON:
{
  "evento_principal": "...",
  "nota_de_impacto": <inteiro 1-10>,
  "justificativa": "...",
  "sentimento": "positivo/negativo/misto/neutro"
}

Notícia:
{conteudo}
""".strip()


@dataclass
class NewsRecord:
    """Structured representation of a single news item."""

    ticker: str
    title: str
    content: str
    published_at: datetime
    source: Optional[str] = None


@dataclass
class StructuredEvent:
    """Output of the LLM event extraction step."""

    event_type: str
    metrics: List[Dict[str, Any]]
    overall_sentiment: str


@dataclass
class ImpactAssessment:
    """Impact score and reasoning provided by the LLM."""

    main_event: str
    impact_score: int
    justification: str
    sentiment: str


@dataclass
class AnalysisArtifacts:
    """Container with the main outputs generated by a full analysis run."""

    dataset: pd.DataFrame
    scored: pd.DataFrame
    summary: pd.DataFrame
    table_paths: Dict[str, Path]
    figure_paths: Dict[str, List[Path]]


class LLMAnalyzer:
    """Wrapper responsible for all language model calls.

    The class gracefully falls back to heuristic rules when the OpenAI SDK is
    not available or when no API key is provided, allowing offline usage for
    testing and unit tests.
    """

    def __init__(self, model: str = "gpt-4.1-mini", api_key: Optional[str] = None) -> None:
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if self.api_key and OpenAI is not None:
            self._client = OpenAI(api_key=self.api_key)
        else:
            self._client = None
            LOGGER.warning(
                "OpenAI API key não configurada ou SDK indisponível. Usando heurísticas simples."
            )

    def _call_llm(self, prompt: str) -> str:
        if not self._client:
            raise RuntimeError("LLM client not available")

        response = self._client.responses.create(
            model=self.model,
            input=[{"role": "user", "content": prompt}],
            max_output_tokens=400,
        )
        # The Responses API returns a complex object; extract the first text block.
        for item in response.output:  # type: ignore[attr-defined]
            if item["type"] == "message":
                for content in item["content"]:
                    if content["type"] == "text":
                        return content["text"]
        # Fallback: the SDK may expose a convenience property
        if hasattr(response, "output_text"):
            return response.output_text  # type: ignore[attr-defined]
        raise RuntimeError("Não foi possível extrair a resposta do LLM")

    # --- Public API -----------------------------------------------------

    def classify_news(self, text: str) -> str:
        try:
            response_text = self._call_llm(MARKET_MOVING_PROMPT.format(conteudo=text))
            return response_text.strip()
        except Exception:
            # Heuristic fallback: check for financial keywords
            lowered = text.lower()
            triggers = [
                "lucro",
                "prejuízo",
                "guidance",
                "demissão",
                "processo",
                "fusao",
                "aquisição",
                "ipo",
                "resultado",
                "recompra",
            ]
            if any(token in lowered for token in triggers):
                return "Market-Moving"
            if "evento" in lowered or "marketing" in lowered:
                return "Fluff/Marketing"
            return "Irrelevante"

    def extract_structured_event(self, text: str) -> StructuredEvent:
        try:
            response_text = self._call_llm(EVENT_EXTRACTION_PROMPT.format(conteudo=text))
            data = json.loads(response_text)
        except Exception:
            data = {
                "evento_tipo": "unknown",
                "metricas": [],
                "sentimento_geral": "neutro",
            }
        return StructuredEvent(
            event_type=data.get("evento_tipo", "unknown"),
            metrics=data.get("metricas", []),
            overall_sentiment=data.get("sentimento_geral", "neutro"),
        )

    def assess_impact(self, text: str) -> ImpactAssessment:
        try:
            response_text = self._call_llm(IMPACT_RATING_PROMPT.format(conteudo=text))
            data = json.loads(response_text)
        except Exception:
            data = {
                "evento_principal": "unknown",
                "nota_de_impacto": 5,
                "justificativa": "Estimativa heurística por ausência do LLM.",
                "sentimento": "neutro",
            }
        score = int(data.get("nota_de_impacto", 5))
        score = max(1, min(score, 10))
        return ImpactAssessment(
            main_event=data.get("evento_principal", "unknown"),
            impact_score=score,
            justification=data.get("justificativa", ""),
            sentiment=data.get("sentimento", "neutro"),
        )


class NewsLoader:
    """Utility responsible for reading and normalising the news datasets."""

    def __init__(self, data_paths: Sequence[Path]) -> None:
        self.data_paths = list(data_paths)

    @staticmethod
    def _normalise_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        candidates = [
            "published_at",
            "date",
            "published_date",
            "datetime",
        ]
        date_col = next((c for c in candidates if c in df.columns), None)
        if date_col is None:
            raise ValueError("Nenhuma coluna de data encontrada no dataset")

        df = df.copy()
        df["published_at"] = df[date_col].apply(NewsLoader._parse_date)

        ticker_col = next((c for c in ("ticker", "symbol", "ativo") if c in df.columns), None)
        if ticker_col is None:
            raise ValueError("Nenhuma coluna de ticker encontrada no dataset")

        content_col = next((c for c in ("content", "body", "texto") if c in df.columns), None)
        if content_col is None:
            raise ValueError("Nenhuma coluna de conteúdo encontrada")

        title_col = next((c for c in ("title", "headline", "titulo") if c in df.columns), None)
        if title_col is None:
            title_col = content_col

        df = df.rename(
            columns={
                ticker_col: "ticker",
                content_col: "content",
                title_col: "title",
            }
        )
        if "source" not in df.columns:
            df["source"] = None
        return df[["ticker", "title", "content", "published_at", "source"]]

    @staticmethod
    def _parse_date(value: Any) -> datetime:
        if isinstance(value, datetime):
            return value
        if isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        if isinstance(value, str):
            return parser.parse(value)
        raise TypeError(f"Tipo de data não suportado: {type(value)}")

    def load(self) -> pd.DataFrame:
        frames: List[pd.DataFrame] = []
        for path in self.data_paths:
            if not path.exists():
                LOGGER.warning("Arquivo %s não encontrado; ignorando", path)
                continue
            df = pd.read_parquet(path)
            frames.append(self._normalise_dataframe(df))
        if not frames:
            raise FileNotFoundError("Nenhum dataset válido foi carregado")
        combined = pd.concat(frames, ignore_index=True)
        combined.dropna(subset=["ticker", "content", "published_at"], inplace=True)
        combined.sort_values("published_at", inplace=True)
        return combined


class DisseminationFeatureBuilder:
    """Implements the FinGPT-style clustering features."""

    def __init__(self, n_features: int = 5000, batch_size: int = 64, random_state: int = 42) -> None:
        self.vectorizer = TfidfVectorizer(max_features=n_features)
        self.batch_size = batch_size
        self.random_state = random_state

    def fit_transform(self, texts: Sequence[str]) -> np.ndarray:
        return self.vectorizer.fit_transform(texts)

    def transform(self, texts: Sequence[str]) -> np.ndarray:
        return self.vectorizer.transform(texts)

    def build_features(
        self,
        df: pd.DataFrame,
        window_minutes: int = 30,
        max_clusters: int = 8,
    ) -> pd.DataFrame:
        if df.empty:
            return pd.DataFrame(columns=[
                "numero_clusters_ativos",
                "tamanho_maior_cluster",
                "velocidade_cluster",
                "sentimento_ponderado_cluster",
            ])

        vectors = self.fit_transform(df["content"].tolist())
        km = MiniBatchKMeans(
            n_clusters=min(max_clusters, max(1, vectors.shape[0] // 2)),
            batch_size=self.batch_size,
            random_state=self.random_state,
        )
        cluster_labels = km.fit_predict(vectors)

        df = df.copy()
        df["cluster"] = cluster_labels
        df.sort_values("published_at", inplace=True)
        features: List[Dict[str, Any]] = []
        window = timedelta(minutes=window_minutes)

        for i, row in df.iterrows():
            start_time = row["published_at"] - window
            mask = (df["published_at"] >= start_time) & (df["published_at"] <= row["published_at"])
            window_df = df.loc[mask]
            cluster_counts = window_df["cluster"].value_counts()
            numero_clusters_ativos = cluster_counts.shape[0]
            tamanho_maior_cluster = cluster_counts.max()

            # Estimate cluster velocity by comparing to previous window
            prev_mask = (df["published_at"] >= start_time - window) & (df["published_at"] < start_time)
            prev_counts = df.loc[prev_mask, "cluster"].value_counts()
            if not prev_counts.empty:
                growth = (cluster_counts.sum() - prev_counts.sum()) / max(prev_counts.sum(), 1)
            else:
                growth = cluster_counts.sum()

            # Weighted sentiment using the impact assessment's sentiment when available
            sentiments = window_df.get("impact_sentiment", pd.Series(["neutro"] * len(window_df)))
            weights = window_df.groupby("cluster").cluster.transform("count")
            sentiment_map = {
                "positivo": 1.0,
                "negativo": -1.0,
                "misto": 0.0,
                "neutro": 0.0,
            }
            sent_values = sentiments.map(sentiment_map).fillna(0.0)
            if weights is not None and weights.sum() > 0:
                weighted_sentiment = float(np.average(sent_values, weights=weights))
            else:
                weighted_sentiment = 0.0

            features.append(
                {
                    "index": i,
                    "numero_clusters_ativos": float(numero_clusters_ativos),
                    "tamanho_maior_cluster": float(tamanho_maior_cluster),
                    "velocidade_cluster": float(growth),
                    "sentimento_ponderado_cluster": weighted_sentiment,
                }
            )

        feature_df = pd.DataFrame(features).set_index("index")
        return feature_df


class FeatureAssembler:
    """Builds the machine-learning ready feature matrix."""

    def __init__(self) -> None:
        self.encoder = OneHotEncoder(handle_unknown="ignore")
        self._fitted = False
        self.feature_names_: List[str] = []

    def fit_transform(self, df: pd.DataFrame) -> np.ndarray:
        categorical_cols = ["evento_tipo", "sentimento_geral", "impact_sentiment"]
        numeric_cols = [
            "impact_score",
            "numero_clusters_ativos",
            "tamanho_maior_cluster",
            "velocidade_cluster",
            "sentimento_ponderado_cluster",
        ]
        metric_features = self._expand_metric_features(df.get("metricas", pd.Series(dtype=object)))
        numeric = df[numeric_cols].fillna(0.0)
        categorical = df[categorical_cols].fillna("desconhecido")
        encoded = self.encoder.fit_transform(categorical).toarray()
        self._fitted = True
        metric_names = [
            "metrics_beats",
            "metrics_misses",
            "metrics_inline",
            "metrics_avg_surprise",
        ]
        encoded_names = self.encoder.get_feature_names_out(categorical_cols).tolist()
        self.feature_names_ = numeric_cols + metric_names + encoded_names
        return np.hstack([numeric.values, metric_features, encoded])

    def transform(self, df: pd.DataFrame) -> np.ndarray:
        if not self._fitted:
            raise RuntimeError("Encoder não foi treinado")
        categorical_cols = ["evento_tipo", "sentimento_geral", "impact_sentiment"]
        numeric_cols = [
            "impact_score",
            "numero_clusters_ativos",
            "tamanho_maior_cluster",
            "velocidade_cluster",
            "sentimento_ponderado_cluster",
        ]
        metric_features = self._expand_metric_features(df.get("metricas", pd.Series(dtype=object)))
        numeric = df[numeric_cols].fillna(0.0)
        categorical = df[categorical_cols].fillna("desconhecido")
        encoded = self.encoder.transform(categorical).toarray()
        return np.hstack([numeric.values, metric_features, encoded])

    @staticmethod
    def _expand_metric_features(metric_series: pd.Series) -> np.ndarray:
        # Flatten metrics into aggregated statistics (count of beats/misses, avg surprise etc.)
        beat_counts = []
        miss_counts = []
        inline_counts = []
        surprises = []
        for metrics in metric_series.fillna([]):
            beats = sum(1 for m in metrics if m.get("resultado") == "beat")
            misses = sum(1 for m in metrics if m.get("resultado") == "miss")
            inline = sum(1 for m in metrics if m.get("resultado") == "in-line")
            surprises.append(FeatureAssembler._average_surprise(metrics))
            beat_counts.append(beats)
            miss_counts.append(misses)
            inline_counts.append(inline)
        return np.vstack([beat_counts, miss_counts, inline_counts, surprises]).T

    @staticmethod
    def _average_surprise(metrics: Sequence[Dict[str, Any]]) -> float:
        values: List[float] = []
        for metric in metrics:
            try:
                actual = float(metric.get("valor")) if metric.get("valor") is not None else None
                expected = float(metric.get("expectativa")) if metric.get("expectativa") is not None else None
            except (TypeError, ValueError):
                actual = expected = None
            if actual is not None and expected not in (None, 0):
                values.append((actual - expected) / abs(expected))
        if not values:
            return 0.0
        return float(np.mean(values))


class PriceFetcher:
    """Downloads price data from yfinance."""

    def __init__(self, auto_adjust: bool = True) -> None:
        if yf is None:
            raise ImportError("yfinance não está instalado. Instale antes de usar o PriceFetcher.")
        self.auto_adjust = auto_adjust

    def get_history(self, ticker: str, start: datetime, end: datetime) -> pd.DataFrame:
        data = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=self.auto_adjust)
        if data.empty:
            raise ValueError(f"Sem dados de preço para {ticker}")
        data = data[["Close"]].rename(columns={"Close": "close"})
        data.index = pd.to_datetime(data.index)
        return data


class XGBoostImpactModel:
    """Wraps an XGBoost regressor to predict future returns."""

    def __init__(self, horizons: Optional[Sequence[str]] = None) -> None:
        self.horizons = tuple(horizons or ("ret_1d", "ret_3d", "ret_5d"))
        self.model_params = dict(
            n_estimators=300,
            max_depth=4,
            learning_rate=0.05,
            subsample=0.7,
            colsample_bytree=0.7,
            objective="reg:squarederror",
        )
        self.features = FeatureAssembler()
        self.models: Dict[str, XGBRegressor] = {}
        self._features_fitted = False

    def reset(self) -> None:
        """Clear trained models and fitted encoders."""

        self.models.clear()
        self.features = FeatureAssembler()
        self._features_fitted = False

    def fit(self, df: pd.DataFrame, target_column: str) -> None:
        if target_column not in df.columns:
            raise ValueError(f"Coluna alvo {target_column} não encontrada")
        valid_mask = df[target_column].notna()
        if not bool(valid_mask.any()):
            raise ValueError(f"Nenhum valor válido para {target_column}")
        target_df = df.loc[valid_mask]
        y = target_df[target_column].values
        if not self._features_fitted:
            X = self.features.fit_transform(target_df)
            self._features_fitted = True
        else:
            X = self.features.transform(target_df)
        model = XGBRegressor(**self.model_params)
        model.fit(X, y)
        self.models[target_column] = model

    def predict(self, df: pd.DataFrame, horizons: Optional[Sequence[str]] = None) -> Dict[str, np.ndarray]:
        if not self._features_fitted or not self.models:
            raise RuntimeError("Modelo ainda não treinado")
        feature_matrix = self.features.transform(df)
        output: Dict[str, np.ndarray] = {}
        selected = horizons or tuple(self.models.keys())
        for horizon in selected:
            if horizon not in self.models:
                continue
            model = self.models[horizon]
            output[f"pred_{horizon}"] = model.predict(feature_matrix)
        return output

    def feature_importance(self, horizon: str) -> pd.Series:
        if horizon not in self.models:
            raise ValueError(f"Horizonte {horizon} não treinado")
        model = self.models[horizon]
        names = getattr(self.features, "feature_names_", None)
        if not names:
            names = [f"feature_{i}" for i in range(model.feature_importances_.shape[0])]
        return pd.Series(model.feature_importances_, index=names).sort_values(ascending=False)


class NewsImpactAnalyzer:
    """High-level orchestration class."""

    def __init__(
        self,
        data_paths: Sequence[Path],
        llm_model: str = "gpt-4.1-mini",
        openai_api_key: Optional[str] = None,
        horizons: Sequence[str] = ("ret_1d", "ret_3d", "ret_5d"),
    ) -> None:
        self.loader = NewsLoader(data_paths)
        self.llm = LLMAnalyzer(model=llm_model, api_key=openai_api_key)
        self.cluster_builder = DisseminationFeatureBuilder()
        self.price_fetcher = PriceFetcher()
        self.horizons = tuple(horizons)
        self.model = XGBoostImpactModel(horizons=self.horizons)

    def prepare_dataset(
        self,
        ticker: str,
        start: Optional[datetime] = None,
        end: Optional[datetime] = None,
    ) -> pd.DataFrame:
        news_df = self.loader.load()
        news_df = news_df[news_df["ticker"].str.upper() == ticker.upper()].copy()
        if start:
            news_df = news_df[news_df["published_at"] >= start]
        if end:
            news_df = news_df[news_df["published_at"] <= end]
        if news_df.empty:
            raise ValueError(f"Nenhuma notícia encontrada para {ticker}")

        records: List[Dict[str, Any]] = []
        for _, row in news_df.iterrows():
            text = f"{row['title']}\n\n{row['content']}"
            classification = self.llm.classify_news(text)
            if classification != "Market-Moving":
                continue
            structured = self.llm.extract_structured_event(text)
            impact = self.llm.assess_impact(text)
            records.append(
                {
                    "ticker": ticker,
                    "title": row["title"],
                    "content": row["content"],
                    "published_at": row["published_at"],
                    "evento_tipo": structured.event_type,
                    "metricas": structured.metrics,
                    "sentimento_geral": structured.overall_sentiment,
                    "impact_score": impact.impact_score,
                    "impact_sentiment": impact.sentiment,
                    "impact_justificativa": impact.justification,
                }
            )

        dataset = pd.DataFrame(records)
        if dataset.empty:
            raise ValueError("Nenhuma notícia market-moving encontrada")

        dissemination_features = self.cluster_builder.build_features(dataset)
        dataset = dataset.join(dissemination_features, how="left")

        price_start = dataset["published_at"].min() - timedelta(days=2)
        price_end = dataset["published_at"].max() + timedelta(days=10)
        price_history = self.price_fetcher.get_history(ticker, price_start, price_end)

        returns = []
        for _, row in dataset.iterrows():
            event_time = row["published_at"].floor("D")
            base_price = self._get_closest_price(price_history, event_time)
            future_returns = {
                "ret_1d": self._compute_return(price_history, event_time, 1, base_price),
                "ret_3d": self._compute_return(price_history, event_time, 3, base_price),
                "ret_5d": self._compute_return(price_history, event_time, 5, base_price),
            }
            returns.append(future_returns)
        returns_df = pd.DataFrame(returns, index=dataset.index)
        dataset = pd.concat([dataset, returns_df], axis=1)
        return dataset

    def train_model(self, dataset: pd.DataFrame, horizon: str = "ret_3d", reset: bool = False) -> None:
        if horizon not in self.horizons:
            raise ValueError("Horizonte inválido")
        if reset:
            self.model.reset()
        self.model.fit(dataset, target_column=horizon)

    def train_all_models(
        self,
        dataset: pd.DataFrame,
        horizons: Optional[Sequence[str]] = None,
        reset: bool = True,
    ) -> None:
        selected = tuple(horizons or self.horizons)
        if reset:
            self.model.reset()
        for horizon in selected:
            try:
                self.model.fit(dataset, target_column=horizon)
            except ValueError as exc:
                LOGGER.warning("Não foi possível treinar o horizonte %s: %s", horizon, exc)

    def score(
        self,
        dataset: pd.DataFrame,
        horizons: Optional[Sequence[str]] = None,
    ) -> pd.DataFrame:
        predictions = self.model.predict(dataset, horizons=horizons)
        result = dataset.copy()
        for name, values in predictions.items():
            result[name] = values
        return result

    @staticmethod
    def summarise_performance(
        scored_dataset: pd.DataFrame,
        horizons: Sequence[str],
    ) -> pd.DataFrame:
        rows: List[Dict[str, Any]] = []
        for horizon in horizons:
            actual_col = horizon
            pred_col = f"pred_{horizon}"
            if actual_col not in scored_dataset.columns or pred_col not in scored_dataset.columns:
                continue
            subset = scored_dataset[[actual_col, pred_col]].dropna()
            if subset.empty:
                continue
            subset_pct = subset.copy()
            subset_pct[actual_col] = subset_pct[actual_col] * 100
            subset_pct[pred_col] = subset_pct[pred_col] * 100
            mae = mean_absolute_error(subset_pct[actual_col], subset_pct[pred_col])
            rmse = math.sqrt(mean_squared_error(subset_pct[actual_col], subset_pct[pred_col]))
            corr = subset_pct[actual_col].corr(subset_pct[pred_col])
            rows.append(
                {
                    "horizonte": horizon,
                    "media_retorno_real_pct": subset_pct[actual_col].mean(),
                    "media_retorno_previsto_pct": subset_pct[pred_col].mean(),
                    "mae_pct": mae,
                    "rmse_pct": rmse,
                    "correlacao": corr,
                }
            )
        return pd.DataFrame(rows)

    def run_full_analysis(
        self,
        ticker: str,
        start: Optional[datetime] = None,
        end: Optional[datetime] = None,
        horizons: Optional[Sequence[str]] = None,
        output_dir: Optional[Path] = None,
        generate_plots: bool = True,
        top_features: int = 15,
    ) -> AnalysisArtifacts:
        dataset = self.prepare_dataset(ticker=ticker, start=start, end=end)
        selected = tuple(horizons or self.horizons)
        self.train_all_models(dataset, horizons=selected, reset=True)
        if not self.model.models:
            raise RuntimeError(
                "Nenhum modelo foi treinado. Verifique se existem retornos válidos para os horizontes informados."
            )
        scored = self.score(dataset, horizons=selected)
        summary = self.summarise_performance(scored, selected)

        table_paths: Dict[str, Path] = {}
        figure_paths: Dict[str, List[Path]] = {}
        prefix = self._build_run_prefix(ticker, start, end, dataset)

        if output_dir is not None:
            output_dir.mkdir(parents=True, exist_ok=True)
            tables_dir = output_dir / "tables"
            tables_dir.mkdir(parents=True, exist_ok=True)

            dataset_path = tables_dir / f"{prefix}_dataset.parquet"
            scored_path = tables_dir / f"{prefix}_scored.parquet"
            summary_path = tables_dir / f"{prefix}_summary.csv"

            dataset.to_parquet(dataset_path)
            scored.to_parquet(scored_path)
            summary.to_csv(summary_path, index=False)

            table_paths = {
                "dataset": dataset_path,
                "scored": scored_path,
                "summary": summary_path,
            }

            if generate_plots:
                figures_dir = output_dir / "figures"
                figures_dir.mkdir(parents=True, exist_ok=True)

                impact_path = figures_dir / f"{prefix}_impact_vs_returns.png"
                impact_figure = self._plot_impact_vs_returns(scored, selected, impact_path)
                if impact_figure is not None:
                    figure_paths.setdefault("impact_vs_returns", []).append(impact_figure)

                predictions_path = figures_dir / f"{prefix}_predicted_vs_real.png"
                predictions_figure = self._plot_predictions(scored, selected, predictions_path)
                if predictions_figure is not None:
                    figure_paths.setdefault("predicted_vs_real", []).append(predictions_figure)

                feature_paths = self._plot_feature_importance(selected, figures_dir, prefix, top_features)
                if feature_paths:
                    figure_paths.setdefault("feature_importance", []).extend(feature_paths)

        return AnalysisArtifacts(
            dataset=dataset,
            scored=scored,
            summary=summary,
            table_paths=table_paths,
            figure_paths=figure_paths,
        )

    @staticmethod
    def _build_run_prefix(
        ticker: str,
        start: Optional[datetime],
        end: Optional[datetime],
        dataset: pd.DataFrame,
    ) -> str:
        def _fmt(value: Optional[datetime], fallback: datetime) -> str:
            ref = value or fallback
            return ref.strftime("%Y%m%d")

        earliest = dataset["published_at"].min()
        latest = dataset["published_at"].max()
        return f"{ticker.upper()}_{_fmt(start, earliest)}_{_fmt(end, latest)}"

    @staticmethod
    def _format_horizon_label(horizon: str) -> str:
        label = horizon.replace("ret_", "Retorno ")
        if label.endswith("d"):
            label = label[:-1] + " dias"
        return label

    @staticmethod
    def _plot_impact_vs_returns(
        scored: pd.DataFrame,
        horizons: Sequence[str],
        output_path: Path,
    ) -> Optional[Path]:
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
        except ImportError as exc:  # pragma: no cover - optional dependency
            LOGGER.warning("Não foi possível gerar gráfico (instale matplotlib e seaborn): %s", exc)
            return None

        sns.set_theme(style="whitegrid")
        rows = len(horizons)
        fig, axes = plt.subplots(rows, 1, figsize=(8, 4 * rows), squeeze=False)
        for ax, horizon in zip(axes.flatten(), horizons):
            if horizon not in scored.columns:
                continue
            subset = scored[["impact_score", horizon]].dropna()
            if subset.empty:
                ax.set_visible(False)
                continue
            subset = subset.copy()
            subset[horizon] = subset[horizon] * 100
            sns.regplot(
                data=subset,
                x="impact_score",
                y=horizon,
                ax=ax,
                scatter_kws={"alpha": 0.6},
                line_kws={"color": "darkorange"},
            )
            ax.set_title(f"Impacto x {NewsImpactAnalyzer._format_horizon_label(horizon)}")
            ax.set_xlabel("Impact score (1-10)")
            ax.set_ylabel("Retorno (%)")
        fig.tight_layout()
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        plt.close(fig)
        return output_path

    @staticmethod
    def _plot_predictions(
        scored: pd.DataFrame,
        horizons: Sequence[str],
        output_path: Path,
    ) -> Optional[Path]:
        try:
            import matplotlib.pyplot as plt
        except ImportError as exc:  # pragma: no cover - optional dependency
            LOGGER.warning("Não foi possível gerar gráfico (instale matplotlib): %s", exc)
            return None

        rows = len(horizons)
        fig, axes = plt.subplots(rows, 1, figsize=(10, 4 * rows), squeeze=False)
        for ax, horizon in zip(axes.flatten(), horizons):
            actual_col = horizon
            pred_col = f"pred_{horizon}"
            if actual_col not in scored.columns or pred_col not in scored.columns:
                ax.set_visible(False)
                continue
            subset = scored[["published_at", actual_col, pred_col]].dropna()
            if subset.empty:
                ax.set_visible(False)
                continue
            subset = subset.sort_values("published_at")
            subset = subset.copy()
            subset[actual_col] = subset[actual_col] * 100
            subset[pred_col] = subset[pred_col] * 100
            ax.plot(subset["published_at"], subset[actual_col], label="Retorno realizado", marker="o")
            ax.plot(subset["published_at"], subset[pred_col], label="Retorno previsto", marker="s")
            ax.set_title(f"{NewsImpactAnalyzer._format_horizon_label(horizon)}")
            ax.set_ylabel("Retorno (%)")
            ax.legend()
        for ax in axes.flatten():
            ax.set_xlabel("Data")
            ax.tick_params(axis="x", rotation=45)
        fig.tight_layout()
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        plt.close(fig)
        return output_path

    def _plot_feature_importance(
        self,
        horizons: Sequence[str],
        figures_dir: Path,
        prefix: str,
        top_features: int,
    ) -> List[Path]:
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
        except ImportError as exc:  # pragma: no cover - optional dependency
            LOGGER.warning("Não foi possível gerar gráfico (instale matplotlib e seaborn): %s", exc)
            return []

        sns.set_theme(style="whitegrid")
        paths: List[Path] = []
        for horizon in horizons:
            try:
                importance = self.model.feature_importance(horizon).head(top_features)
            except ValueError:
                continue
            if importance.empty:
                continue
            fig, ax = plt.subplots(figsize=(8, max(4, top_features * 0.3)))
            importance.iloc[::-1].plot.barh(ax=ax, color="steelblue")
            ax.set_title(f"Importância de features - {self._format_horizon_label(horizon)}")
            ax.set_xlabel("Importância (gain)")
            fig.tight_layout()
            path = figures_dir / f"{prefix}_feature_importance_{horizon}.png"
            fig.savefig(path, dpi=150, bbox_inches="tight")
            plt.close(fig)
            paths.append(path)
        return paths

    # ------------------------------------------------------------------

    @staticmethod
    def _get_closest_price(price_history: pd.DataFrame, date: datetime) -> float:
        # Attempt to get the closing price on the date; fallback to next available
        if date in price_history.index:
            return float(price_history.loc[date, "close"])
        future_prices = price_history[price_history.index >= date]
        if future_prices.empty:
            raise ValueError(f"Sem preço disponível após {date}")
        return float(future_prices.iloc[0]["close"])

    @staticmethod
    def _compute_return(
        price_history: pd.DataFrame,
        event_time: datetime,
        days_ahead: int,
        base_price: float,
    ) -> float:
        target_date = event_time + timedelta(days=days_ahead)
        if target_date in price_history.index:
            future_price = float(price_history.loc[target_date, "close"])
        else:
            future_prices = price_history[price_history.index >= target_date]
            if future_prices.empty:
                return np.nan
            future_price = float(future_prices.iloc[0]["close"])
        if base_price in (0, np.nan):
            return np.nan
        return (future_price - base_price) / base_price


def build_default_analyzer(
    data_dir: Path,
    llm_model: str = "gpt-4.1-mini",
    openai_api_key: Optional[str] = None,
    horizons: Sequence[str] = ("ret_1d", "ret_3d", "ret_5d"),
) -> NewsImpactAnalyzer:
    """Convenience factory that points to the default parquet files."""

    paths = [
        data_dir / "investing_news.parquet",
        data_dir / "investing_news_nacionais.parquet",
        data_dir / "investing_news_nacionais_que_faltaram.parquet",
    ]
    return NewsImpactAnalyzer(paths, llm_model=llm_model, openai_api_key=openai_api_key, horizons=horizons)


__all__ = [
    "NewsRecord",
    "StructuredEvent",
    "ImpactAssessment",
    "AnalysisArtifacts",
    "LLMAnalyzer",
    "NewsLoader",
    "DisseminationFeatureBuilder",
    "FeatureAssembler",
    "PriceFetcher",
    "XGBoostImpactModel",
    "NewsImpactAnalyzer",
    "build_default_analyzer",
]


def _parse_cli_datetime(value: Optional[str]) -> Optional[datetime]:
    if value in (None, "", "null", "None"):
        return None
    return parser.parse(value)


def _parse_cli_horizons(value: Optional[str]) -> Sequence[str]:
    if not value:
        return ("ret_1d", "ret_3d", "ret_5d")
    return tuple(h.strip() for h in value.split(",") if h.strip())


def main(argv: Optional[Sequence[str]] = None) -> None:
    """Command-line entry point for batch execution."""

    cli = argparse.ArgumentParser(description="Analisa o impacto de notícias sobre um ticker")
    cli.add_argument("--ticker", required=True, help="Ticker a ser analisado (ex.: AAPL)")
    cli.add_argument("--start", help="Data inicial (YYYY-mm-dd)")
    cli.add_argument("--end", help="Data final (YYYY-mm-dd)")
    cli.add_argument("--data-dir", default="data", help="Diretório que contém os arquivos investing_news*.parquet")
    cli.add_argument("--output-dir", default="reports", help="Diretório onde os relatórios serão salvos")
    cli.add_argument("--horizons", default="ret_1d,ret_3d,ret_5d", help="Horizontes de retorno separados por vírgula")
    cli.add_argument("--llm-model", default="gpt-4.1-mini", help="Modelo de LLM a ser utilizado")
    cli.add_argument("--openai-api-key", help="Chave de API do OpenAI (opcional)")
    cli.add_argument("--no-plots", action="store_true", help="Não gerar gráficos")
    cli.add_argument("--top-features", type=int, default=15, help="Número de features exibidas nos gráficos de importância")

    args = cli.parse_args(argv)

    horizons = _parse_cli_horizons(args.horizons)
    analyzer = build_default_analyzer(
        Path(args.data_dir),
        llm_model=args.llm_model,
        openai_api_key=args.openai_api_key,
        horizons=horizons,
    )
    artifacts = analyzer.run_full_analysis(
        ticker=args.ticker,
        start=_parse_cli_datetime(args.start),
        end=_parse_cli_datetime(args.end),
        horizons=horizons,
        output_dir=Path(args.output_dir),
        generate_plots=not args.no_plots,
        top_features=args.top_features,
    )

    if artifacts.table_paths:
        print("Tabelas geradas:")
        for name, path in artifacts.table_paths.items():
            print(f" - {name}: {path}")
    else:
        print("Nenhum arquivo de tabela foi salvo (output_dir não fornecido).")

    if artifacts.figure_paths:
        print("Gráficos gerados:")
        for name, paths in artifacts.figure_paths.items():
            for path in paths:
                print(f" - {name}: {path}")
    else:
        print("Nenhum gráfico gerado.")

    if not artifacts.summary.empty:
        print("\nResumo de desempenho:")
        print(artifacts.summary)


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
